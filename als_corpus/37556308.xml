<?xml version="1.0" ?>
<!DOCTYPE PubmedArticleSet PUBLIC "-//NLM//DTD PubMedArticle, 1st January 2023//EN" "https://dtd.nlm.nih.gov/ncbi/pubmed/out/pubmed_230101.dtd">
<PubmedArticleSet>
<PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Automated"><PMID Version="1">37556308</PMID><DateCompleted><Year>2023</Year><Month>08</Month><Day>18</Day></DateCompleted><DateRevised><Year>2023</Year><Month>10</Month><Day>10</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Electronic">1558-9102</ISSN><JournalIssue CitedMedium="Internet"><Volume>66</Volume><Issue>8S</Issue><PubDate><Year>2023</Year><Month>Aug</Month><Day>17</Day></PubDate></JournalIssue><Title>Journal of speech, language, and hearing research : JSLHR</Title><ISOAbbreviation>J Speech Lang Hear Res</ISOAbbreviation></Journal><ArticleTitle>Wav2DDK: Analytical and Clinical Validation of an Automated Diadochokinetic Rate Estimation Algorithm on Remotely Collected Speech.</ArticleTitle><Pagination><StartPage>3166</StartPage><EndPage>3181</EndPage><MedlinePgn>3166-3181</MedlinePgn></Pagination><ELocationID EIdType="doi" ValidYN="Y">10.1044/2023_JSLHR-22-00282</ELocationID><Abstract><AbstractText Label="PURPOSE">Oral diadochokinesis is a useful task in assessment of speech motor function in the context of neurological disease. Remote collection of speech tasks provides a convenient alternative to in-clinic visits, but scoring these assessments can be a laborious process for clinicians. This work describes Wav2DDK, an automated algorithm for estimating the diadochokinetic (DDK) rate on remotely collected audio from healthy participants and participants with amyotrophic lateral sclerosis (ALS).</AbstractText><AbstractText Label="METHOD">Wav2DDK was developed using a corpus of 970 DDK assessments from healthy and ALS speakers where ground truth DDK rates were provided manually by trained annotators. The clinical utility of the algorithm was demonstrated on a corpus of 7,919 assessments collected longitudinally from 26 healthy controls and 82 ALS speakers. Corpora were collected via the participants' own mobile device, and instructions for speech elicitation were provided via a mobile app. DDK rate was estimated by parsing the character transcript from a deep neural network transformer acoustic model trained on healthy and ALS speech.</AbstractText><AbstractText Label="RESULTS">Algorithm estimated DDK rates are highly accurate, achieving .98 correlation with manual annotation, and an average error of only 0.071 syllables per second. The rate exactly matched ground truth for 83% of files and was within 0.5 syllables per second for 95% of files. Estimated rates achieve a high test-retest reliability (<i>r</i> = .95) and show good correlation with the revised ALS functional rating scale speech subscore (<i>r</i> = .67).</AbstractText><AbstractText Label="CONCLUSION">We demonstrate a system for automated DDK estimation that increases efficiency of calculation beyond manual annotation. Thorough analytical and clinical validation demonstrates that the algorithm is not only highly accurate, but also provides a convenient, clinically relevant metric for tracking longitudinal decline in ALS, serving to promote participation and diversity of participants in clinical research.</AbstractText><AbstractText Label="SUPPLEMENTAL MATERIAL">https://doi.org/10.23641/asha.23787033.</AbstractText></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Kadambi</LastName><ForeName>Prad</ForeName><Initials>P</Initials><Identifier Source="ORCID">0000-0003-0634-1394</Identifier><AffiliationInfo><Affiliation>School of Electrical, Computer and Energy Engineering, Arizona State University, Tempe.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Aural Analytics Inc., Tempe, AZ.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Stegmann</LastName><ForeName>Gabriela M</ForeName><Initials>GM</Initials><AffiliationInfo><Affiliation>Aural Analytics Inc., Tempe, AZ.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Liss</LastName><ForeName>Julie</ForeName><Initials>J</Initials><Identifier Source="ORCID">0000-0001-8782-2901</Identifier><AffiliationInfo><Affiliation>School of Speech and Hearing Science, Arizona State University, Tempe.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Aural Analytics Inc., Tempe, AZ.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Berisha</LastName><ForeName>Visar</ForeName><Initials>V</Initials><Identifier Source="ORCID">0000-0001-8804-8874</Identifier><AffiliationInfo><Affiliation>School of Electrical, Computer and Energy Engineering, Arizona State University, Tempe.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>School of Speech and Hearing Science, Arizona State University, Tempe.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Aural Analytics Inc., Tempe, AZ.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Hahn</LastName><ForeName>Shira</ForeName><Initials>S</Initials><Identifier Source="ORCID">0000-0002-1148-7665</Identifier><AffiliationInfo><Affiliation>Aural Analytics Inc., Tempe, AZ.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><GrantList CompleteYN="Y"><Grant><GrantID>R01 DC006859</GrantID><Acronym>DC</Acronym><Agency>NIDCD NIH HHS</Agency><Country>United States</Country></Grant><Grant><GrantID>R21 DC019475</GrantID><Acronym>DC</Acronym><Agency>NIDCD NIH HHS</Agency><Country>United States</Country></Grant></GrantList><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType><PublicationType UI="D052061">Research Support, N.I.H., Extramural</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>08</Month><Day>09</Day></ArticleDate></Article><MedlineJournalInfo><Country>United States</Country><MedlineTA>J Speech Lang Hear Res</MedlineTA><NlmUniqueID>9705610</NlmUniqueID><ISSNLinking>1092-4388</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D006801" MajorTopicYN="N">Humans</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D013060" MajorTopicYN="Y">Speech</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D000690" MajorTopicYN="Y">Amyotrophic Lateral Sclerosis</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D015203" MajorTopicYN="N">Reproducibility of Results</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D013062" MajorTopicYN="N">Speech Articulation Tests</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D000465" MajorTopicYN="N">Algorithms</DescriptorName></MeshHeading></MeshHeadingList></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="pmc-release"><Year>2024</Year><Month>2</Month><Day>1</Day></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>8</Month><Day>18</Day><Hour>6</Hour><Minute>43</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>8</Month><Day>9</Day><Hour>18</Hour><Minute>42</Minute></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>8</Month><Day>9</Day><Hour>12</Hour><Minute>23</Minute></PubMedPubDate></History><PublicationStatus>ppublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">37556308</ArticleId><ArticleId IdType="pmc">PMC10555468</ArticleId><ArticleId IdType="doi">10.1044/2023_JSLHR-22-00282</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Antolik, T. K., &amp; Fougeron, C. (2013). Consonant distortion in dysarthria due to Parkinson's disease, amyotrophic lateral sclerosis and cerebellar ataxia. In Bimbot F. (Ed.), Interspeech 2013 (pp. 2152&#x2013;2156). International Speech Communication Association. 10.21437/Interspeech.2013-509</Citation><ArticleIdList><ArticleId IdType="doi">10.21437/Interspeech.2013-509</ArticleId></ArticleIdList></Reference><Reference><Citation>Baevski, A., Zhou, Y., Mohamed, A., &amp; Auli, M. (2020). wav2vec 2.0: A framework for self-supervised learning of speech representations. Advances in Neural Information Processing Systems, 33, 12449&#x2013;12460.</Citation></Reference><Reference><Citation>Bansal, S., Kamper, H., Livescu, K., Lopez, A., &amp; Goldwater, S. (2018). Pre-training on high-resource speech recognition improves low-resource speech-to-text translation. arXiv preprint arXiv:1809.01431. 10.21437/Interspeech.2018-1326</Citation><ArticleIdList><ArticleId IdType="doi">10.21437/Interspeech.2018-1326</ArticleId></ArticleIdList></Reference><Reference><Citation>Icht, M., &amp; Ben-David, B. M. (2014). Oral-diadochokinesis rates across languages: English and Hebrew norms. Journal of Communication Disorders, 48, 27&#x2013;37. 10.1016/j.jcomdis.2014.02.002</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jcomdis.2014.02.002</ArticleId><ArticleId IdType="pubmed">24630145</ArticleId></ArticleIdList></Reference><Reference><Citation>Ben-David B. M., &amp; Icht M. (2016) Oral-diadochokinetic rates for Hebrew-speaking healthy aging population: Non-word versus real-word repetition. International Journal of Language and Communication Disorders, 52(3), 301&#x2013;310. 10.1111/1460-6984.12272</Citation><ArticleIdList><ArticleId IdType="doi">10.1111/1460-6984.12272</ArticleId><ArticleId IdType="pubmed">27432555</ArticleId></ArticleIdList></Reference><Reference><Citation>Ben-David, B. M., &amp; Icht, M. (2018). The effect of practice and visual feedback on oral-diadochokinetic rates for younger and older adults. Language and Speech, 61(1), 113&#x2013;134. 10.1177/0023830917708808</Citation><ArticleIdList><ArticleId IdType="doi">10.1177/0023830917708808</ArticleId><ArticleId IdType="pubmed">28610466</ArticleId></ArticleIdList></Reference><Reference><Citation>Berisha, V., Krantsevich, C., Hahn, P. R., Hahn, S., Dasarathy, G., Turaga, P., &amp; Liss, J. (2021). Digital medicine and the curse of dimensionality. NPJ digital medicine, 4(1), 1&#x2013;8. 10.1038/s41746-021-00521-5</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41746-021-00521-5</ArticleId><ArticleId IdType="pmc">PMC8553745</ArticleId><ArticleId IdType="pubmed">34711924</ArticleId></ArticleIdList></Reference><Reference><Citation>Braley, M., Pierce, J. S., Saxena, S., De Oliveira, E., Taraboanta, L., Anantha, V., Lakhan, S. E., &amp; Kiran, S. (2021). A virtual, randomized, control trial of a digital therapeutic for speech, language, and cognitive intervention in post-stroke persons with aphasia. Frontiers in Neurology, 12, 626780. 10.3389/fneur.2021.626780</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fneur.2021.626780</ArticleId><ArticleId IdType="pmc">PMC7907641</ArticleId><ArticleId IdType="pubmed">33643204</ArticleId></ArticleIdList></Reference><Reference><Citation>Cedarbaum, J. M., Stambler, N., Malta, E., Fuller, C., Hilt, D., Thurmond, B., &amp; Nakanishi, A. (1999). The ALSFRS-R: A revised ALS functional rating scale that incorporates assessments of respiratory function. Journal of the Neurological Sciences, 169(1&#x2013;2), 13&#x2013;21. 10.1016/S0022-510X(99)00210-5</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/S0022-510X(99)00210-5</ArticleId><ArticleId IdType="pubmed">10540002</ArticleId></ArticleIdList></Reference><Reference><Citation>Choe, J., &amp; Han, J. S. (1998). Diadochokinetic rate of normal children and adults: A preliminary study. Communication Sciences &amp; Disorders, 3(1), 183&#x2013;194.</Citation></Reference><Reference><Citation>Devlin, J., Chang, M. W., Lee, K., &amp; Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805</Citation></Reference><Reference><Citation>Fletcher, S. G. (1972). Time-by-count measurement of diadochokinetic syllable rate. Journal of Speech and Hearing Research, 15(4), 763&#x2013;770. 10.1044/jshr.1504.763</Citation><ArticleIdList><ArticleId IdType="doi">10.1044/jshr.1504.763</ArticleId><ArticleId IdType="pubmed">4652397</ArticleId></ArticleIdList></Reference><Reference><Citation>Goldsack, J. C., Coravos, A., Bakker, J. P., Bent, B., Dowling, A. V., Fitzer-Attas, C., Godfrey, A., Godino, J. G., Gujar, N., Izmailova, E., Manta, C., Peterson, B., Vandendriessche, B., Wood, W. A., Wang, K. W., &amp; Dunn, J. (2020). Verification, analytical validation, and clinical validation (V3): The foundation of determining fit-for-purpose for Biometric Monitoring Technologies (BioMeTs). NPJ Digital Medicine, 3(1), 1&#x2013;15. 10.1038/s41746-020-0260-4</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41746-020-0260-4</ArticleId><ArticleId IdType="pmc">PMC7156507</ArticleId><ArticleId IdType="pubmed">32337371</ArticleId></ArticleIdList></Reference><Reference><Citation>Graves, A., Fern&#xe1;ndez, S., Gomez, F., &amp; Schmidhuber, J. (2006, June). Connectionist temporal classification: Labeling unsegmented sequence data with recurrent neural networks. In Cohen W. &amp; Moore A. (Eds.), Proceedings of the 23rd International Conference on Machine Learning (pp. 369&#x2013;376). Association for Computing Machinery. 10.1145/1143844.1143891</Citation><ArticleIdList><ArticleId IdType="doi">10.1145/1143844.1143891</ArticleId></ArticleIdList></Reference><Reference><Citation>He K., Zhang X., Ren S., &amp; Sun J. (2016, June). Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (pp. 770&#x2013;778). 10.1109/CVPR.2016.90</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/CVPR.2016.90</ArticleId></ArticleIdList></Reference><Reference><Citation>Hendrycks, D., Liu, X., Wallace, E., Dziedzic, A., Krishnan, R., &amp; Song, D. (2020). Pretrained transformers improve out-of-distribution robustness. arXiv preprint arXiv:2004.06100. 10.18653/v1/2020.acl-main.244</Citation><ArticleIdList><ArticleId IdType="doi">10.18653/v1/2020.acl-main.244</ArticleId></ArticleIdList></Reference><Reference><Citation>Karlsson, F., Schalling, E., Laakso, K., Johansson, K., &amp; Hartelius, L. (2020). Assessment of speech impairment in patients with Parkinson's disease from acoustic quantifications of oral diadochokinetic sequences. The Journal of the Acoustical Society of America, 147(2), 839&#x2013;851. 10.1121/10.0000581</Citation><ArticleIdList><ArticleId IdType="doi">10.1121/10.0000581</ArticleId><ArticleId IdType="pubmed">32113309</ArticleId></ArticleIdList></Reference><Reference><Citation>Kent, R. D. (1996). Hearing and believing: Some limits to the auditory-perceptual assessment of speech and voice disorders. American Journal of Speech-Language Pathology, 5(3), 7&#x2013;23. 10.1044/1058-0360.0503.07</Citation><ArticleIdList><ArticleId IdType="doi">10.1044/1058-0360.0503.07</ArticleId></ArticleIdList></Reference><Reference><Citation>Kent, R. D., Kim, Y., &amp; Chen, L. M. (2022). Oral and laryngeal diadochokinesis across the life span: A scoping review of methods, reference data, and clinical applications. Journal of Speech, Language, and Hearing Research, 65(2), 574&#x2013;623. 10.1044/2021_JSLHR-21-00396</Citation><ArticleIdList><ArticleId IdType="doi">10.1044/2021_JSLHR-21-00396</ArticleId><ArticleId IdType="pubmed">34958599</ArticleId></ArticleIdList></Reference><Reference><Citation>Li, J., Deng, L., Gong, Y. &amp; Haeb-Umbach, R. (2014, April). An overview of noise-robust automatic speech recognition. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 22(4), 745&#x2013;777. 10.1109/TASLP.2014.2304637</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TASLP.2014.2304637</ArticleId></ArticleIdList></Reference><Reference><Citation>Liss, J. M., White, L., Mattys, S. L., Lansford, K., Lotto, A. J., Spitzer, S. M., &amp; Caviness, J. N. (2009). Quantifying speech rhythm abnormalities in the dysarthrias. Journal of Speech, Language, and Hearing Research, 52(5), 1334&#x2013;1352. 10.1044/1092-4388(2009/08-0208)</Citation><ArticleIdList><ArticleId IdType="doi">10.1044/1092-4388(2009/08-0208)</ArticleId><ArticleId IdType="pmc">PMC3738185</ArticleId><ArticleId IdType="pubmed">19717656</ArticleId></ArticleIdList></Reference><Reference><Citation>Mundt, J. C., Snyder, P. J., Cannizzaro, M. S., Chappie, K., &amp; Geralts, D. S. (2007). Voice acoustic measures of depression severity and treatment response collected via interactive voice response (IVR) technology. Journal of Neurolinguistics, 20(1), 50&#x2013;64. 10.1016/j.jneuroling.2006.04.001</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jneuroling.2006.04.001</ArticleId><ArticleId IdType="pmc">PMC3022333</ArticleId><ArticleId IdType="pubmed">21253440</ArticleId></ArticleIdList></Reference><Reference><Citation>Notley, S., &amp; Magdon-Ismail M. (2018). Examining the use of neural networks for feature extraction: A comparative analysis using deep learning, support vector machines, and k-nearest neighbor classifiers. arXiv preprint arXiv:1805.02294v2.</Citation></Reference><Reference><Citation>Novotny, M., Melechovsky, J., Rozenstoks, K., Tykalova, T., Kryze, P., Kanok, M., Klempir, J., &amp; Rusz, J. (2020). Comparison of automated acoustic methods for oral diadochokinesis assessment in amyotrophic lateral sclerosis. Journal of Speech, Language, and Hearing Research: JSLHR, 63(10), 3453&#x2013;3460. 10.1044/2020_JSLHR-20-00109</Citation><ArticleIdList><ArticleId IdType="doi">10.1044/2020_JSLHR-20-00109</ArticleId><ArticleId IdType="pubmed">32955982</ArticleId></ArticleIdList></Reference><Reference><Citation>Orozco-Arroyave, J. R., V&#xe1;squez-Correa, J. C., Vargas-Bonilla, J. F., Arora, R., Dehak, N., Nidadavolu, P. S., Christensen, H., Rudzicz, F., Yancheva, M., Chinaei, H., Vann, A., Vogler, N., Bocklet, T., Cernak, M., Hannink, J., &amp; N&#xf6;th, E. (2018). NeuroSpeech. SoftwareX, 8, 69&#x2013;70. 10.1016/j.softx.2017.08.004</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.softx.2017.08.004</ArticleId></ArticleIdList></Reference><Reference><Citation>Panayotov, V., Chen, G., Povey, D., &amp; Khudanpur, S. (2015, April). Librispeech: An asr corpus based on public domain audio books. In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP) (pp. 5206&#x2013;5210). IEEE. 10.1109/ICASSP.2015.7178964</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ICASSP.2015.7178964</ArticleId></ArticleIdList></Reference><Reference><Citation>Prathanee, B., Thanaviratananich, S., &amp; Pongjanyakul, A. (2003). Oral diadochokinetic rates for normal Thai children. International Journal of Language &amp; Communication Disorders, 38(4), 417&#x2013;428. 10.1080/1368282031000154042</Citation><ArticleIdList><ArticleId IdType="doi">10.1080/1368282031000154042</ArticleId><ArticleId IdType="pubmed">14578054</ArticleId></ArticleIdList></Reference><Reference><Citation>Rozenstoks, K., Novotny, M., Horakova, D., &amp; Rusz, J. (2020, January). Automated assessment of oral diadochokinesis in multiple sclerosis using a neural network approach: Effect of different syllable repetition paradigms. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 28(1), 32&#x2013;41. 10.1109/TNSRE.2019.2943064</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TNSRE.2019.2943064</ArticleId><ArticleId IdType="pubmed">31545738</ArticleId></ArticleIdList></Reference><Reference><Citation>Rong, P., Yunusova, Y., Wang, J., &amp; Green, J. R. (2015). Predicting early bulbar decline in amyotrophic lateral sclerosis: A speech subsystem approach. Behavioral Neurology, 2015, Article ID 183027. 10.1155/2015/183027</Citation><ArticleIdList><ArticleId IdType="doi">10.1155/2015/183027</ArticleId><ArticleId IdType="pmc">PMC4468279</ArticleId><ArticleId IdType="pubmed">26136624</ArticleId></ArticleIdList></Reference><Reference><Citation>Rong, P., Yunusova, Y., Wang, J., Zinman, L., Pattee, G. L., Berry, J. D., Perry, B., &amp; Green, J. R. (2016). Predicting speech intelligibility decline in amyotrophic lateral sclerosis based on the deterioration of individual speech subsystems. PLOS ONE, 11(5), Article e0154971. 10.1371/journal.pone.0154971</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pone.0154971</ArticleId><ArticleId IdType="pmc">PMC4858181</ArticleId><ArticleId IdType="pubmed">27148967</ArticleId></ArticleIdList></Reference><Reference><Citation>Rong P. (2020). Automated acoustic analysis of oral diadochokinesis to assess bulbar motor involvement in amyotrophic lateral sclerosis. Journal of Speech, Language, and Hearing Research, 63(1), 59&#x2013;73. 10.1044/2019_JSLHR-19-00178</Citation><ArticleIdList><ArticleId IdType="doi">10.1044/2019_JSLHR-19-00178</ArticleId><ArticleId IdType="pubmed">31940257</ArticleId></ArticleIdList></Reference><Reference><Citation>Rutkove, S. B., Qi, K., Shelton, K., Liss, J., Berisha, V., &amp; Shefner, J. M. (2019). ALS longitudinal studies with frequent data collection at home: Study design and baseline data. Amyotrophic Lateral Sclerosis &amp; Frontotemporal Degeneration, 20(1&#x2013;2), 61&#x2013;67. 10.1080/21678421.2018.1541095</Citation><ArticleIdList><ArticleId IdType="doi">10.1080/21678421.2018.1541095</ArticleId><ArticleId IdType="pmc">PMC6513689</ArticleId><ArticleId IdType="pubmed">30486680</ArticleId></ArticleIdList></Reference><Reference><Citation>Segal, Y., Hitczenko K., Goldrick M., Buchwald A., Roberts, A., &amp; Keshet, J. (2022). DDKtor: Automatic Diadochokinetic Speech Analysis. 10.21437/Interspeech.2022-311</Citation><ArticleIdList><ArticleId IdType="doi">10.21437/Interspeech.2022-311</ArticleId></ArticleIdList></Reference><Reference><Citation>Shah, J., Singla, Y. K., Chen, C., &amp; Shah, R. R. (2021). What all do audio transformer models hear? Probing acoustic representations for language delivery and its structure. arXiv preprint arXiv:2101.00387. 10.1109/ICDMW58026.2022.00120</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ICDMW58026.2022.00120</ArticleId></ArticleIdList></Reference><Reference><Citation>Sinha, P., Vandana, V. P., Lewis, N. V., Jayaram, M., &amp; Enderby, P. (2015). Evaluating the effect of risperidone on speech: A cross-sectional study. Asian Journal of Psychiatry, 15, 51&#x2013;55. 10.1016/j.ajp.2015.05.005</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ajp.2015.05.005</ArticleId><ArticleId IdType="pubmed">26013669</ArticleId></ArticleIdList></Reference><Reference><Citation>Sm&#xe9;kal, Z., Mekyska, J., Rektorov&#xe1;, I., &amp; Fa&#xfa;ndez-Zanuy, M. (2013). Analysis of neurological disorders based on digital processing of speech and handwritten text. International Symposium on Signals, Circuits and Systems (ISSCS), 1&#x2013;6. 10.1109/ISSCS.2013.6651178</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ISSCS.2013.6651178</ArticleId></ArticleIdList></Reference><Reference><Citation>Srinivasan, V., Ramalingam, V., &amp; Arulmozhi, P. (2014). Artificial neural network based pathological voice classification using MFCC features. International Journal of Science, Environment and Technology, 3(1), 291&#x2013;302.</Citation></Reference><Reference><Citation>Stegmann, G. M., Hahn, S., Liss, J., Shefner, J., Rutkove, S., Shelton, K., Duncan, C. J., &amp; Berisha, V. (2020). Early detection and tracking of bulbar changes in ALS via frequent and remote speech analysis. NPJ Digital Medicine, 3(1), 1&#x2013;5. 10.1038/s41746-020-00335-x</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41746-020-00335-x</ArticleId><ArticleId IdType="pmc">PMC7555482</ArticleId><ArticleId IdType="pubmed">33083567</ArticleId></ArticleIdList></Reference><Reference><Citation>Stipancic, K. L., Yunusova, Y., Berry, J. D., &amp; Green, J. R. (2018). Minimally detectable change and minimal clinically important difference of a decline in sentence intelligibility and speaking rate for individuals with amyotrophic lateral sclerosis. Journal of Speech, Language, and Hearing Research, 61(11), 2757&#x2013;2771. 10.1044/2018_JSLHR-S-17-0366</Citation><ArticleIdList><ArticleId IdType="doi">10.1044/2018_JSLHR-S-17-0366</ArticleId><ArticleId IdType="pmc">PMC6693567</ArticleId><ArticleId IdType="pubmed">30383220</ArticleId></ArticleIdList></Reference><Reference><Citation>Tafiadis, D., Zarokanellou, V., Prentza, A., Voniati, L., &amp; Ziavra, N. (2021). Oral diadochokinetic rates for real words and non-words in Greek-speaking children. Open Linguistics, 7(1), 722&#x2013;738. 10.1515/opli-2020-0178</Citation><ArticleIdList><ArticleId IdType="doi">10.1515/opli-2020-0178</ArticleId></ArticleIdList></Reference><Reference><Citation>Tafiadis, D., Zarokanellou, V., Prentza, A., Voniati, L., &amp; Ziavra, N. (2022). Diadochokinetic rates in healthy young and elderly Greek-speaking adults: The effect of types of stimuli. International Journal of Language &amp; Communication Disorders. 10.1111/1460-6984.12747</Citation><ArticleIdList><ArticleId IdType="doi">10.1111/1460-6984.12747</ArticleId><ArticleId IdType="pubmed">35703470</ArticleId></ArticleIdList></Reference><Reference><Citation>Tanchip, C., Guarin, D. L., McKinlay, S., Barnett, C., Kalra, S., Genge, A., Korngut, L., Green, J. R., Berry, J., Zinman, L., Yadollahi, A., Abrahao, A., &amp; Yunusova, Y. (2022). Validating automatic diadochokinesis analysis methods across dysarthria severity and syllable task in amyotrophic lateral sclerosis. Journal of Speech, Language, and Hearing Research, 65(3), 940&#x2013;953. 10.1044/2021_JSLHR-21-00503</Citation><ArticleIdList><ArticleId IdType="doi">10.1044/2021_JSLHR-21-00503</ArticleId><ArticleId IdType="pmc">PMC9150739</ArticleId><ArticleId IdType="pubmed">35171700</ArticleId></ArticleIdList></Reference><Reference><Citation>Tao, F., Daudet L., Poellabauer C., Schneider S. L., &amp; Busso C. (2016). A portable automatic PA-TA-KA syllable detection system to derive biomarkers for neurological disorders. In Interspeech (pp. 362&#x2013;366). 10.21437/Interspeech.2016-789</Citation><ArticleIdList><ArticleId IdType="doi">10.21437/Interspeech.2016-789</ArticleId></ArticleIdList></Reference><Reference><Citation>Tjaden, K., &amp; Watling, E. (2003). Characteristics of diadochokinesis in multiple sclerosis and Parkinson's disease. Folia Phoniatrica et Logopaedica, 55(5), 241&#x2013;259. 10.1159/000072155</Citation><ArticleIdList><ArticleId IdType="doi">10.1159/000072155</ArticleId><ArticleId IdType="pubmed">12931058</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang, Y. T., Gao, K., Zhao, Y., Kuruvilla-Dugdale, M., Lever, T. E., &amp; Bunyak, F. (2019). DeepDDK: A deep learning based oral-diadochokinesis analysis software. IEEE-EMBS International Conference on Biomedical and Health Informatics, 2019, 1&#x2013;4. 10.1109/BHI.2019.8834506</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/BHI.2019.8834506</ArticleId><ArticleId IdType="pmc">PMC7451101</ArticleId><ArticleId IdType="pubmed">32864624</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang, Y. T., Kent, R. D., Duffy, J. R., Thomas, J. E., &amp; Weismer, G. (2004). Alternating motion rate as an index of speech motor disorder in traumatic brain injury. Clinical Linguistics &amp; Phonetics, 18(1), 57&#x2013;84. 10.1080/02699200310001596160</Citation><ArticleIdList><ArticleId IdType="doi">10.1080/02699200310001596160</ArticleId><ArticleId IdType="pubmed">15053268</ArticleId></ArticleIdList></Reference><Reference><Citation>Zamani, P., Rezai, H., &amp; Garmatani, N. T. (2017). Meaningful words and non-words repetitive articulatory rate (oral diadochokinesis) in Persian speaking children. Journal of Psycholinguistic Research, 46(4), 897&#x2013;904. 10.1007/s10936-016-9469-4</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10936-016-9469-4</ArticleId><ArticleId IdType="pmc">PMC5511617</ArticleId><ArticleId IdType="pubmed">28025805</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhu, J., Zhang, C., &amp; Jurgens, D. (2022, May). Phone-to-audio alignment without text: A semi-supervised approach. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 8167&#x2013;8171). IEEE. 10.1109/ICASSP43922.2022.9746112</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ICASSP43922.2022.9746112</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle></PubmedArticleSet>